{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "from torchvision import transforms, models\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import nn, optim\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, f1_score\n",
        "from sklearn.model_selection import LeaveOneOut\n",
        "\n",
        "# Load dataset\n",
        "train_data_path = '/content/emnist-byclass-train.csv'\n",
        "val_data_path = '/content/emnist-byclass-test.csv'\n",
        "\n",
        "data_train = pd.read_csv(train_data_path, header=None, nrows=50)\n",
        "data_val = pd.read_csv(val_data_path, header=None, nrows=20)\n",
        "print(\"Dataset loaded successfully.\")\n",
        "\n",
        "# Preprocessing function for raw pixel data\n",
        "def preprocess_image(data):\n",
        "    data = np.clip(data, 0, 255).astype(np.uint8).reshape(28, 28)\n",
        "    return Image.fromarray(data).convert(\"RGB\")\n",
        "\n",
        "# Custom dataset class\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, dataframe, transform=None):\n",
        "        self.dataframe = dataframe\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        label = self.dataframe.iloc[idx, 0]\n",
        "        img_data = self.dataframe.iloc[idx, 1:].values\n",
        "        image = preprocess_image(img_data)\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "# Transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Define device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Leave-One-Out Cross Validation (LOOCV)\n",
        "loo = LeaveOneOut()\n",
        "data_array = data_train.to_numpy()\n",
        "all_preds, all_labels = [], []\n",
        "\n",
        "print(\"Starting LOOCV...\")\n",
        "for train_idx, test_idx in tqdm(loo.split(data_array), total=len(data_array)):\n",
        "    train_samples = data_array[train_idx]\n",
        "    test_sample = data_array[test_idx]\n",
        "\n",
        "    train_dataset = CustomDataset(pd.DataFrame(train_samples), transform=transform)\n",
        "    test_dataset = CustomDataset(pd.DataFrame(test_sample), transform=transform)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "    # Initialize model\n",
        "    model = models.alexnet(pretrained=True)\n",
        "    model.classifier[6] = nn.Linear(4096, 62)  # Adjust to number of classes\n",
        "    model.to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "    # Training loop\n",
        "    model.train()\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device).long()\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device).long()\n",
        "            outputs = model(inputs)\n",
        "            all_preds.append(torch.argmax(outputs, dim=1).cpu().item())\n",
        "            all_labels.append(labels.cpu().item())\n",
        "\n",
        "# Evaluation metrics\n",
        "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
        "accuracy = accuracy_score(all_labels, all_preds)\n",
        "precision = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
        "f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "\n",
        "print(\"\\nEvaluation Results:\")\n",
        "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CjCCkaswg17h",
        "outputId": "74a37ffb-5826-4357-ab35-e4ef2a6b239a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded successfully.\n",
            "Starting LOOCV...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/50 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n",
            "\n",
            "  0%|          | 0.00/233M [00:00<?, ?B/s]\u001b[A\n",
            "  0%|          | 1.12M/233M [00:00<00:21, 11.3MB/s]\u001b[A\n",
            "  3%|▎         | 6.50M/233M [00:00<00:06, 37.1MB/s]\u001b[A\n",
            "  9%|▊         | 20.0M/233M [00:00<00:02, 83.9MB/s]\u001b[A\n",
            " 15%|█▍        | 34.8M/233M [00:00<00:01, 111MB/s] \u001b[A\n",
            " 21%|██        | 49.2M/233M [00:00<00:01, 126MB/s]\u001b[A\n",
            " 27%|██▋       | 63.6M/233M [00:00<00:01, 134MB/s]\u001b[A\n",
            " 34%|███▎      | 78.4M/233M [00:00<00:01, 141MB/s]\u001b[A\n",
            " 39%|███▉      | 91.9M/233M [00:00<00:01, 139MB/s]\u001b[A\n",
            " 45%|████▌     | 105M/233M [00:00<00:01, 117MB/s] \u001b[A\n",
            " 50%|█████     | 117M/233M [00:01<00:01, 119MB/s]\u001b[A\n",
            " 55%|█████▌    | 129M/233M [00:01<00:00, 120MB/s]\u001b[A\n",
            " 60%|██████    | 141M/233M [00:01<00:00, 116MB/s]\u001b[A\n",
            " 65%|██████▌   | 152M/233M [00:01<00:00, 109MB/s]\u001b[A\n",
            " 70%|██████▉   | 163M/233M [00:01<00:00, 101MB/s]\u001b[A\n",
            " 75%|███████▌  | 175M/233M [00:01<00:00, 109MB/s]\u001b[A\n",
            " 80%|████████  | 187M/233M [00:01<00:00, 113MB/s]\u001b[A\n",
            " 85%|████████▌ | 199M/233M [00:01<00:00, 115MB/s]\u001b[A\n",
            " 90%|█████████ | 210M/233M [00:01<00:00, 114MB/s]\u001b[A\n",
            "100%|██████████| 233M/233M [00:02<00:00, 115MB/s]\n",
            "100%|██████████| 50/50 [19:49<00:00, 23.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluation Results:\n",
            "Confusion Matrix:\n",
            "[[0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 2 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 1 2 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
            " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
            "Accuracy: 0.0800\n",
            "Precision: 0.0101\n",
            "F1-Score: 0.0153\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}